---
title: "H2O Deployment"
date: today
author: James Bristow
fig-pos: 'H'
toc: true
lof: true
lot: true
link-citations: true
---

# Introduction

This tutorial will demonstrate how one may fit and deploy an H2O machine learning model onto Kubernetes via vetiver. We first load all required dependencies.

```{r setup, include = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = FALSE, output=TRUE, eval=TRUE}

box::use(../../R/tidymodels_deployment)
box::use(
  readr[read_csv, read_file],
  here[here],
  rsample[initial_split, training, testing],
  parsnip[auto_ml, set_engine, set_mode],
  recipes[recipe, update_role, step_zv, step_nzv, step_normalize, step_impute_knn, update, all_numeric, all_outcomes],
  hardhat[tune],
  workflows[workflow, add_recipe, add_model, fit],
  vetiver[vetiver_model, vetiver_pin_write, vetiver_api, vetiver_pin_read],
  pins[board_temp, board_s3, pin_write, pin_read, pin_upload, pin_download],
  readr[read_file],
  paws.common,
  paws.storage,
  plotly[ggplotly],
  probably[int_conformal_split, cal_plot_regression],
  plumber[pr, pr_run],
  zeallot[...],
  targets[tar_make],
  h2o[h2o.connect],
  agua[extract_fit_parsnip, rank_results, member_weights],
  dplyr[filter, arrange, mutate],
  ggplot2[ggplot, geom_boxplot, labs, scale_x_sqrt, aes],
  tidyr[unnest]
)

h2o.connect(
  ip = "h2o.k8s.dev.co.nz", port = 443, https = TRUE
)
```

# Data

We next load the mtcars dataset, and perform data pre-processing.

```{r load-h2o-data, include = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=TRUE}

data("mtcars")

data_split <- mtcars |>
  initial_split(
    prop = 0.9
  )

data_train <- training(data_split)
data_test  <- testing(data_split)

training_recipe <- recipe(data_train) |>
  update_role(everything(), new_role = "predictor") |>
  update_role(mpg, new_role = "outcome") |>
  step_zv() |>
  step_nzv() |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_impute_knn()
```

# AutoML

We next train an ensemble using AutoML.

```{r h2o-automl, include = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=TRUE}

auto_spec <-
  auto_ml() |>
  set_engine("h2o", max_runtime_secs = 10, seed = 1) |>
  set_mode("regression")

auto_wflow <-
  workflow() |>
  add_model(auto_spec) |>
  add_recipe(training_recipe)

auto_fit <- fit(auto_wflow, data = data_train)
extract_fit_parsnip(auto_fit)

```

## Results

We next view the results of the AutoML algorithm.

```{r h2o-automl-results, include = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=TRUE}

rank_results(auto_fit) |>
  filter(.metric == "mae") |>
  arrange(rank)

```

```{r fig-h2o-local, results = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=TRUE}
#| fig-cap: H20 Ensemble importances.

p <- auto_fit |>
  extract_fit_parsnip() |>
  member_weights() |>
  unnest(importance) |>
  filter(type == "scaled_importance") |>
  ggplot() +
  geom_boxplot(aes(value, algorithm)) +
  scale_x_sqrt() +
  labs(y = NULL, x = "scaled importance", title = "Member importance in stacked ensembles")

ggplotly(p)
```

# Conformalisation

We next conformalise the AutoML model to produce non-parametric prediction intervals.

```{r h2o-conformalisation, include = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=TRUE}
conformalised_stack <- int_conformal_split(auto_fit, data_train)
conformalised_stack$training <- data_train
predict(conformalised_stack, new_data = data_test)
```

```{r fig-conformal-ensemble-work-calibration, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=TRUE}
#| fig-cap: Calibration plot of observed and predicted discrepancies on testing data.

p <- conformalised_stack |>
  predict(new_data = data_test, level = 0.95) |>
  mutate(
    mpg = data_test$mpg
  ) |>
  cal_plot_regression(truth = mpg, estimate = .pred) 

ggplotly(p)
```

# Model deployment

Let's save the model to S3. Then deploy it.

```{r h2o-s3, include = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=FALSE}

board <- board_s3(
  "data", 
  access_key = "user", 
  secret_access_key = "password", 
  region = "us-east-2",
  endpoint = "https://minio-api.k8s.dev.co.nz"
)

vetiver_create_description.int_conformal_split <- function(model) {
    "A conformalised machine learning model."
}

vetiver_ptype.int_conformal_split <- function(model, ...) {
    vctrs::vec_ptype(model$training)
}

handler_predict.int_conformal_split <- function(vetiver_model, ...) {
    
    ptype <- vetiver_model$prototype
    
    function(req) {
        newdata <- req$body
        newdata <- vetiver::vetiver_type_convert(newdata, ptype)
        newdata <- hardhat::scream(newdata, ptype)
        ret <- predict(vetiver_model$model, new_data = newdata, ...)
        list(.pred = ret)
    }
    
}

vetiver_create_meta.int_conformal_split <- function(model, metadata) {
    vetiver::vetiver_meta(metadata, required_pkgs = "probably")
}

v_conformalised_stack <- vetiver_model(conformalised_stack, "conformalised_h2o_stack")
board |> 
  vetiver_pin_write(v_conformalised_stack)

v_model <- board |> vetiver_pin_read("conformalised_h2o_stack")

pr(here("book", "h2o_deployment/app/plumber.R")) |>
  vetiver_api(v_model) |>
  pr_run(port = 8088)
```

# Run with targets

We can run the above as a targets pipeline.

```{r tidymodels-targets, include = TRUE, echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, output=TRUE, eval=FALSE}

tar_make(
  script = here("pipelines", "_targets_h2o_deployment.R")
)
```
